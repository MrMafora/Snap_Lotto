#!/usr/bin/env python3
"""
Phase 2 Neural Network Lottery Prediction System
Advanced AI using Random Forest, Neural Network Simulation, and Ensemble Models
"""

import os
import psycopg2
import numpy as np
import pandas as pd
import logging
import json
from datetime import datetime, timedelta
from collections import Counter
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, accuracy_score

logger = logging.getLogger(__name__)

class NeuralNetworkSimulator:
    """Simulated neural network using NumPy for lottery prediction"""
    
    def __init__(self, input_size=20, hidden_size=50, output_size=6):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # Initialize weights and biases
        np.random.seed(42)  # For reproducible results
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))
        
        # Learning parameters
        self.learning_rate = 0.01
        self.iterations = 1000
        
    def sigmoid(self, z):
        """Sigmoid activation function"""
        return 1 / (1 + np.exp(-np.clip(z, -250, 250)))  # Prevent overflow
    
    def sigmoid_derivative(self, z):
        """Derivative of sigmoid function"""
        s = self.sigmoid(z)
        return s * (1 - s)
    
    def forward_propagation(self, X):
        """Forward pass through the network"""
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self.sigmoid(self.z2)
        return self.a2
    
    def backward_propagation(self, X, y, output):
        """Backward pass to update weights"""
        m = X.shape[0]
        
        # Calculate gradients
        dz2 = output - y
        dW2 = (1/m) * np.dot(self.a1.T, dz2)
        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)
        
        dz1 = np.dot(dz2, self.W2.T) * self.sigmoid_derivative(self.z1)
        dW1 = (1/m) * np.dot(X.T, dz1)
        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)
        
        # Update weights
        self.W2 -= self.learning_rate * dW2
        self.b2 -= self.learning_rate * db2
        self.W1 -= self.learning_rate * dW1
        self.b1 -= self.learning_rate * db1
    
    def train(self, X, y):
        """Train the neural network"""
        for i in range(self.iterations):
            output = self.forward_propagation(X)
            self.backward_propagation(X, y, output)
            
            if i % 100 == 0:
                loss = np.mean(np.square(output - y))
                logger.info(f"Neural Network Iteration {i}, Loss: {loss:.6f}")
    
    def predict(self, X):
        """Make predictions"""
        return self.forward_propagation(X)

class AdvancedLotteryPredictor:
    """Phase 2 Advanced AI Lottery Prediction System"""
    
    def __init__(self):
        self.models = {}
        self.model_weights = {
            'random_forest': 0.4,
            'gradient_boost': 0.3,
            'neural_network': 0.3
        }
        self.scaler = StandardScaler()
        self.neural_net = None
        
    def get_historical_data_enhanced(self, cur, lottery_type, days_back=365):
        """Get enhanced historical data for neural network training"""
        try:
            cur.execute('''
                SELECT main_numbers, bonus_numbers, draw_date, draw_number
                FROM lottery_results
                WHERE lottery_type = %s 
                  AND draw_date >= CURRENT_DATE - make_interval(days => %s)
                ORDER BY draw_date ASC, draw_number ASC
                LIMIT 200
            ''', (lottery_type, days_back))
            
            results = cur.fetchall()
            
            if len(results) < 10:
                logger.warning(f"Insufficient data for {lottery_type}: only {len(results)} draws")
                return None, None, None
                
            # Prepare features and targets
            features = []
            targets = []
            all_numbers = []
            
            for i, (main_nums, bonus_nums, draw_date, draw_number) in enumerate(results[:-1]):  # Exclude last for target alignment
                # Parse main numbers
                parsed_main = self._parse_numbers(main_nums)
                if not parsed_main:
                    continue
                    
                all_numbers.extend(parsed_main)
                
                # Create features from historical patterns
                feature_vector = self._create_feature_vector(parsed_main, i, results[:i])
                if len(feature_vector) == 20:  # Ensure consistent feature size
                    # Target is next draw's numbers (normalized)
                    next_nums = self._parse_numbers(results[i+1][0])
                    if next_nums and len(next_nums) > 0:
                        # Normalize target numbers to 0-1 range
                        target = [n / 52.0 for n in next_nums]  # Assuming max 52
                        # Pad or truncate to fixed size
                        while len(target) < 6:
                            target.append(0)
                        
                        features.append(feature_vector)
                        targets.append(target[:6])
            
            logger.info(f"Neural Network Training Data: {len(features)} samples for {lottery_type}")
            return np.array(features), np.array(targets), all_numbers
            
        except Exception as e:
            logger.error(f"Error getting enhanced historical data: {e}")
            return None, None, None
    
    def _parse_numbers(self, numbers_data):
        """Parse lottery numbers from database format"""
        try:
            if isinstance(numbers_data, str):
                if numbers_data.startswith('{') and numbers_data.endswith('}'):
                    # PostgreSQL array format
                    nums_str = numbers_data.strip('{}')
                    if nums_str:
                        return [int(x.strip()) for x in nums_str.split(',')]
                else:
                    # JSON format
                    return json.loads(numbers_data)
            elif isinstance(numbers_data, list):
                return numbers_data
            return []
        except:
            return []
    
    def _create_feature_vector(self, current_numbers, position, historical_results):
        """Create a 20-dimensional feature vector for neural network"""
        features = []
        
        # Feature 1-6: Current numbers (normalized)
        features.extend([n / 52.0 for n in current_numbers])
        while len(features) < 6:
            features.append(0)
        
        # Feature 7-8: Statistical measures
        features.append(np.mean(current_numbers) / 52.0)  # Mean
        features.append(np.std(current_numbers) / 52.0)   # Standard deviation
        
        # Feature 9-10: Even/odd ratio
        even_count = sum(1 for n in current_numbers if n % 2 == 0)
        features.append(even_count / len(current_numbers))
        features.append((len(current_numbers) - even_count) / len(current_numbers))
        
        # Feature 11-15: Recent frequency patterns (last 5 draws)
        recent_numbers = []
        for i, (main_nums, _, _, _) in enumerate(historical_results[:5]):
            parsed = self._parse_numbers(main_nums)
            recent_numbers.extend(parsed)
        
        recent_freq = Counter(recent_numbers)
        for i in range(5):
            if i < len(current_numbers):
                freq = recent_freq.get(current_numbers[i], 0)
                features.append(freq / max(1, len(recent_numbers)))
            else:
                features.append(0)
        
        # Feature 16-20: Pattern indicators
        features.append(position / 200.0)  # Position in sequence
        features.append(len(set(current_numbers)) / len(current_numbers))  # Uniqueness
        
        # Consecutive numbers
        consecutive_count = 0
        sorted_nums = sorted(current_numbers)
        for i in range(len(sorted_nums) - 1):
            if sorted_nums[i+1] - sorted_nums[i] == 1:
                consecutive_count += 1
        features.append(consecutive_count / max(1, len(current_numbers)))
        
        # High/low distribution
        high_count = sum(1 for n in current_numbers if n > 26)
        features.append(high_count / len(current_numbers))
        
        # Sum parity
        features.append((sum(current_numbers) % 2))
        
        return features[:20]  # Ensure exactly 20 features
    
    def train_models(self, features, targets, lottery_type):
        """Train all AI models"""
        try:
            if features is None or len(features) < 10:
                logger.warning(f"Insufficient training data for {lottery_type}")
                return False
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                features, targets, test_size=0.2, random_state=42
            )
            
            # Scale features
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)
            
            # Train separate models for each number position
            self.models[f'{lottery_type}_random_forest'] = []
            self.models[f'{lottery_type}_gradient_boost'] = []
            
            logger.info(f"Training ensemble models for {lottery_type} (6 positions)...")
            
            for position in range(6):  # Train one model per number position
                # Train Random Forest for this position
                rf_model = RandomForestRegressor(
                    n_estimators=50, 
                    max_depth=8, 
                    random_state=42 + position,
                    n_jobs=1  # Reduce parallel jobs to avoid memory issues
                )
                rf_model.fit(X_train_scaled, y_train[:, position])
                self.models[f'{lottery_type}_random_forest'].append(rf_model)
                
                # Train Gradient Boosting for this position
                gb_model = GradientBoostingRegressor(
                    n_estimators=50, 
                    max_depth=4, 
                    random_state=42 + position
                )
                gb_model.fit(X_train_scaled, y_train[:, position])
                self.models[f'{lottery_type}_gradient_boost'].append(gb_model)
            
            # Train Neural Network (can handle multi-output)
            logger.info(f"Training Neural Network for {lottery_type}...")
            self.neural_net = NeuralNetworkSimulator(
                input_size=20, 
                hidden_size=30, 
                output_size=6
            )
            self.neural_net.train(X_train_scaled, y_train)
            self.models[f'{lottery_type}_neural_network'] = self.neural_net
            
            # Evaluate models
            self._evaluate_models(X_test_scaled, y_test, lottery_type)
            
            logger.info(f"âœ… All models trained successfully for {lottery_type}")
            return True
            
        except Exception as e:
            logger.error(f"Error training models for {lottery_type}: {e}")
            return False
    
    def _evaluate_models(self, X_test, y_test, lottery_type):
        """Evaluate model performance and adjust weights"""
        try:
            scores = {}
            
            # Evaluate Random Forest (ensemble of position models)
            if f'{lottery_type}_random_forest' in self.models:
                rf_predictions = []
                for position, model in enumerate(self.models[f'{lottery_type}_random_forest']):
                    pos_pred = model.predict(X_test)
                    rf_predictions.append(pos_pred)
                rf_pred = np.column_stack(rf_predictions)
                rf_mse = mean_squared_error(y_test, rf_pred)
                scores['random_forest'] = 1 / (1 + rf_mse)
            
            # Evaluate Gradient Boosting (ensemble of position models)
            if f'{lottery_type}_gradient_boost' in self.models:
                gb_predictions = []
                for position, model in enumerate(self.models[f'{lottery_type}_gradient_boost']):
                    pos_pred = model.predict(X_test)
                    gb_predictions.append(pos_pred)
                gb_pred = np.column_stack(gb_predictions)
                gb_mse = mean_squared_error(y_test, gb_pred)
                scores['gradient_boost'] = 1 / (1 + gb_mse)
            
            # Evaluate Neural Network
            if self.neural_net is not None:
                nn_pred = self.neural_net.predict(X_test)
                nn_mse = mean_squared_error(y_test, nn_pred)
                scores['neural_network'] = 1 / (1 + nn_mse)
            
            # Normalize weights based on performance
            total_score = sum(scores.values())
            if total_score > 0:
                self.model_weights = {
                    model: score / total_score 
                    for model, score in scores.items()
                }
            
            logger.info(f"Model Performance for {lottery_type}:")
            for model_name, score in scores.items():
                mse = 1/score - 1
                weight = self.model_weights.get(model_name, 0)
                logger.info(f"{model_name.title()}: MSE={mse:.6f}, Weight={weight:.3f}")
            
        except Exception as e:
            logger.error(f"Error evaluating models: {e}")
    
    def predict_ensemble(self, feature_vector, lottery_type, number_range=(1, 52), count=6):
        """Generate prediction using ensemble of all models"""
        try:
            if not feature_vector or len(feature_vector) != 20:
                logger.warning("Invalid feature vector for neural network prediction")
                return self._fallback_prediction(number_range, count)
            
            # Scale features
            feature_scaled = self.scaler.transform([feature_vector])
            
            predictions = []
            
            # Get predictions from all models
            if f'{lottery_type}_random_forest' in self.models:
                rf_predictions = []
                for model in self.models[f'{lottery_type}_random_forest']:
                    pos_pred = model.predict(feature_scaled)[0]
                    rf_predictions.append(pos_pred)
                predictions.append(('random_forest', np.array(rf_predictions)))
            
            if f'{lottery_type}_gradient_boost' in self.models:
                gb_predictions = []
                for model in self.models[f'{lottery_type}_gradient_boost']:
                    pos_pred = model.predict(feature_scaled)[0]
                    gb_predictions.append(pos_pred)
                predictions.append(('gradient_boost', np.array(gb_predictions)))
            
            if f'{lottery_type}_neural_network' in self.models:
                nn_pred = self.models[f'{lottery_type}_neural_network'].predict(feature_scaled)[0]
                predictions.append(('neural_network', nn_pred))
            
            if not predictions:
                return self._fallback_prediction(number_range, count)
            
            # Ensemble prediction with weighted voting
            ensemble_pred = np.zeros(6)
            total_weight = 0
            
            for model_name, pred in predictions:
                weight = self.model_weights.get(model_name, 0.33)
                ensemble_pred += weight * pred
                total_weight += weight
            
            if total_weight > 0:
                ensemble_pred /= total_weight
            
            # Convert to lottery numbers
            final_numbers = []
            min_num, max_num = number_range
            
            for pred_val in ensemble_pred[:count]:
                # Denormalize and convert to integer
                number = int(pred_val * max_num)
                number = max(min_num, min(max_num, number))
                
                # Ensure uniqueness
                while number in final_numbers:
                    number = (number % max_num) + 1
                    if number < min_num:
                        number = min_num
                
                final_numbers.append(number)
            
            # Fill remaining slots if needed
            while len(final_numbers) < count:
                remaining = [n for n in range(min_num, max_num + 1) if n not in final_numbers]
                if remaining:
                    final_numbers.append(np.random.choice(remaining))
                else:
                    break
            
            return sorted(final_numbers[:count])
            
        except Exception as e:
            logger.error(f"Error in ensemble prediction: {e}")
            return self._fallback_prediction(number_range, count)
    
    def _fallback_prediction(self, number_range, count):
        """Fallback to simple random prediction"""
        min_num, max_num = number_range
        return sorted(np.random.choice(range(min_num, max_num + 1), count, replace=False))
    
    def get_bonus_data_enhanced(self, cur, lottery_type):
        """Get bonus number historical data for training"""
        try:
            cur.execute('''
                SELECT bonus_numbers, draw_date, draw_number
                FROM lottery_results
                WHERE lottery_type = %s 
                  AND bonus_numbers IS NOT NULL
                  AND draw_date >= CURRENT_DATE - make_interval(days => 365)
                ORDER BY draw_date ASC, draw_number ASC
                LIMIT 100
            ''', (lottery_type,))
            
            results = cur.fetchall()
            
            if len(results) < 5:
                logger.warning(f"Insufficient bonus data for {lottery_type}: only {len(results)} draws")
                return None, None, None
                
            features = []
            targets = []
            all_bonus = []
            
            for i, (bonus_nums, draw_date, draw_number) in enumerate(results[:-1]):
                parsed_bonus = self._parse_numbers(bonus_nums)
                if not parsed_bonus:
                    continue
                    
                all_bonus.extend(parsed_bonus)
                
                # Simple feature vector for bonus prediction (last 5 features)
                feature_vector = [
                    parsed_bonus[0] / 20.0,  # Normalized bonus number
                    i / len(results),         # Position in sequence
                    (sum(parsed_bonus) % 2),  # Parity
                    len(parsed_bonus),        # Count
                    draw_number % 7           # Day of week proxy
                ]
                
                # Pad to 5 features
                while len(feature_vector) < 5:
                    feature_vector.append(0)
                
                features.append(feature_vector[:5])
                
                # Target is next bonus number
                next_bonus = self._parse_numbers(results[i+1][0])
                if next_bonus:
                    targets.append(next_bonus[0] / 20.0)  # Normalized
            
            logger.info(f"Bonus Training Data: {len(features)} samples for {lottery_type}")
            return np.array(features), np.array(targets), all_bonus
            
        except Exception as e:
            logger.error(f"Error getting bonus data: {e}")
            return None, None, None
    
    def train_bonus_models(self, features, targets, lottery_type):
        """Train bonus-specific models"""
        try:
            if len(features) < 5:
                return False
                
            X_train, X_test, y_train, y_test = train_test_split(
                features, targets, test_size=0.2, random_state=42
            )
            
            # Train simple Random Forest for bonus
            bonus_model = RandomForestRegressor(
                n_estimators=30, 
                max_depth=5, 
                random_state=42
            )
            bonus_model.fit(X_train, y_train)
            self.models[f'{lottery_type}_bonus'] = bonus_model
            
            logger.info(f"âœ… Bonus model trained for {lottery_type}")
            return True
            
        except Exception as e:
            logger.error(f"Error training bonus models: {e}")
            return False
    
    def predict_bonus_ensemble(self, feature_vector, lottery_type, bonus_range, bonus_count):
        """Predict bonus numbers using trained model"""
        try:
            if f'{lottery_type}_bonus' not in self.models:
                return None
                
            # Create simple feature vector for bonus
            simple_feature = feature_vector[:5] if len(feature_vector) >= 5 else feature_vector + [0] * (5 - len(feature_vector))
            
            # Predict normalized bonus
            bonus_model = self.models[f'{lottery_type}_bonus']
            predicted_normalized = bonus_model.predict([simple_feature])[0]
            
            # Denormalize to bonus range
            min_bonus, max_bonus = bonus_range
            predicted_bonus = int(predicted_normalized * max_bonus)
            predicted_bonus = max(min_bonus, min(max_bonus, predicted_bonus))
            
            return [predicted_bonus]
            
        except Exception as e:
            logger.warning(f"Error predicting bonus: {e}")
            return None

    def calculate_ensemble_confidence(self, predictions_agreement, model_count, data_quality):
        """Calculate confidence based on model agreement and data quality"""
        try:
            # Base confidence
            base_confidence = 50
            
            # Agreement bonus (models predicting similar numbers)
            agreement_bonus = predictions_agreement * 15
            
            # Model count bonus
            model_bonus = (model_count - 1) * 5
            
            # Data quality bonus
            data_bonus = min(data_quality / 20, 10)
            
            confidence = base_confidence + agreement_bonus + model_bonus + data_bonus
            return max(50, min(85, int(confidence)))  # Cap between 50-85%
            
        except Exception as e:
            logger.warning(f"Error calculating ensemble confidence: {e}")
            return 65

def neural_network_prediction(lottery_type, config):
    """Main function to generate neural network prediction"""
    try:
        predictor = AdvancedLotteryPredictor()
        
        # Connect to database
        conn = psycopg2.connect(os.environ.get('DATABASE_URL'))
        cur = conn.cursor()
        
        logger.info(f"ðŸ§  Generating Neural Network prediction for {lottery_type}...")
        
        # Get training data
        features, targets, all_numbers = predictor.get_historical_data_enhanced(cur, lottery_type)
        
        if features is None:
            logger.warning(f"No training data available for {lottery_type}")
            return None, None, None, None
        
        # Train models
        success = predictor.train_models(features, targets, lottery_type)
        
        if not success:
            logger.warning(f"Model training failed for {lottery_type}")
            return None, None, None, None
        
        # Create feature vector for current prediction
        if all_numbers and len(all_numbers) > 0:
            recent_numbers = all_numbers[-30:] if len(all_numbers) >= 30 else all_numbers
            feature_nums = recent_numbers[-6:] if len(recent_numbers) >= 6 else recent_numbers
            current_feature = predictor._create_feature_vector(feature_nums, 0, [])
        else:
            logger.warning(f"No historical numbers available for {lottery_type}")
            return None, None, None, None
        
        # Generate ensemble prediction for main numbers
        main_prediction = predictor.predict_ensemble(
            current_feature, 
            lottery_type,
            config['main_range'], 
            config['main_count']
        )
        
        # Generate bonus numbers using Neural Network if needed
        bonus_prediction = None
        if config.get('bonus_count', 0) > 0:
            logger.info(f"ðŸŽ¯ Generating Neural Network bonus prediction for {lottery_type}...")
            try:
                # Get bonus historical data for training
                bonus_features, bonus_targets, bonus_numbers = predictor.get_bonus_data_enhanced(cur, lottery_type)
                
                if bonus_features is not None and len(bonus_features) >= 10:
                    # Train bonus-specific model
                    bonus_success = predictor.train_bonus_models(bonus_features, bonus_targets, lottery_type)
                    
                    if bonus_success:
                        bonus_prediction = predictor.predict_bonus_ensemble(
                            current_feature,
                            lottery_type, 
                            config['bonus_range'],
                            config['bonus_count']
                        )
                        logger.info(f"âœ… Neural Network bonus prediction: {bonus_prediction}")
                    else:
                        logger.info(f"âš ï¸ Bonus model training failed, will use Phase 1 for bonus")
                else:
                    logger.info(f"âš ï¸ Insufficient bonus data ({len(bonus_features) if bonus_features is not None else 0} samples), will use Phase 1 for bonus")
                        
            except Exception as e:
                logger.warning(f"âš ï¸ Bonus prediction failed: {e}, will use Phase 1 for bonus")
        
        # Calculate confidence
        confidence = predictor.calculate_ensemble_confidence(
            0.7,  # Assume good agreement for now
            3,    # Three models
            len(features)  # Data quality
        )
        
        # Create detailed reasoning
        reasoning_parts = [
            f"Neural Network ensemble prediction using {len(features)} training samples",
            "Random Forest + Gradient Boosting + Neural Network models",
            f"Advanced pattern recognition with {len(all_numbers)} historical numbers analyzed"
        ]
        
        if bonus_prediction:
            reasoning_parts.append("Including Neural Network bonus prediction")
        elif config.get('bonus_count', 0) > 0:
            reasoning_parts.append("Bonus numbers via Phase 1 fallback")
            
        reasoning = " | ".join(reasoning_parts)
        
        cur.close()
        conn.close()
        
        logger.info(f"âœ… Neural Network prediction complete: {main_prediction} + {bonus_prediction} (confidence: {confidence}%)")
        return main_prediction, bonus_prediction, confidence, reasoning
        
    except Exception as e:
        logger.error(f"Neural network prediction failed: {e}")
        return None, None, None, None

if __name__ == "__main__":
    # Test the neural network system
    logging.basicConfig(level=logging.INFO)
    
    test_config = {'main_count': 6, 'main_range': (1, 52)}
    prediction, confidence, reasoning = neural_network_prediction('LOTTO', test_config)
    
    if prediction:
        print(f"Neural Network Prediction: {prediction}")
        print(f"Confidence: {confidence}%") 
        print(f"Reasoning: {reasoning}")
    else:
        print("Neural network prediction failed")