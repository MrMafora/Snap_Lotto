import os
import logging
import time
import asyncio
import subprocess
import sys
import json
from datetime import datetime
from playwright.async_api import async_playwright
from models import Screenshot, db
import html_parser

logger = logging.getLogger(__name__)

# Create directory for content if it doesn't exist (we'll store HTML content here)
CONTENT_DIR = os.path.join(os.getcwd(), 'content')
os.makedirs(CONTENT_DIR, exist_ok=True)

# Ensure Playwright browsers are installed
def ensure_playwright_browsers():
    try:
        logger.info("Checking Playwright browsers installation...")
        from playwright.sync_api import sync_playwright
        
        # Test if browsers are installed by creating a temporary browser instance
        with sync_playwright() as p:
            try:
                browser = p.chromium.launch()
                browser.close()
                logger.info("Playwright browsers are already installed")
                return True
            except Exception as e:
                logger.warning(f"Playwright browser check failed: {str(e)}")
                return False
    except Exception as e:
        logger.warning(f"Could not import sync_playwright: {str(e)}")
        return False

# Try to install browsers if they're not already installed
if not ensure_playwright_browsers():
    try:
        logger.info("Installing Playwright browsers...")
        subprocess.run([sys.executable, "-m", "playwright", "install", "chromium"], check=True)
        logger.info("Playwright browsers installed successfully")
    except Exception as e:
        logger.error(f"Failed to install Playwright browsers: {str(e)}")
        # Continue anyway, as they might be available through Replit's environment

async def capture_html_content_async(url):
    """
    Capture the HTML content of the specified URL using Playwright.
    This uses a full browser instance to properly render JavaScript and bypass anti-scraping measures.
    
    Args:
        url (str): The URL to capture
        
    Returns:
        tuple: (filepath, html_content) or (None, None) if failed
    """
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{url.split('/')[-1]}.html"
        filepath = os.path.join(CONTENT_DIR, filename)
        
        logger.info(f"Capturing HTML content from {url} using Playwright")
        
        # Use Playwright to capture HTML content
        try:
            # Set browser executable path to use the system installed Chromium
            chromium_path = subprocess.run(
                ["which", "chromium"], 
                capture_output=True, 
                text=True
            ).stdout.strip()
            
            logger.info(f"Using Chromium from: {chromium_path}")
            
            async with async_playwright() as p:
                # Launch browser with memory optimization flags
                try:
                    browser = await p.chromium.launch(
                        headless=True,
                        executable_path=chromium_path if chromium_path else None,
                        args=['--disable-gpu', '--disable-dev-shm-usage', '--no-zygote', 
                             '--no-sandbox', '--single-process']
                    )
                except Exception as e:
                    logger.warning(f"Failed to launch with specific path: {str(e)}, trying default installation")
                    browser = await p.chromium.launch(
                        headless=True,
                        args=['--disable-gpu', '--disable-dev-shm-usage', '--no-zygote', 
                             '--no-sandbox', '--single-process']
                    )
                
                # Set up context with desktop viewport and realistic user agent
                context = await browser.new_context(
                    viewport={'width': 1366, 'height': 768},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                )
                
                # Create page and navigate to URL
                page = await context.new_page()
                
                # Set extra headers to appear more like a real browser
                await page.set_extra_http_headers({
                    "Accept-Language": "en-US,en;q=0.9",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
                })
                
                # Navigate to the target URL with reasonable timeout
                await page.goto(url, wait_until='load', timeout=60000)
                
                # Wait for the page to render completely 
                await page.wait_for_timeout(5000)
                
                # Get the fully rendered HTML content
                html_content = await page.content()
                
                # Save the HTML content to a file
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(html_content)
                
                # Close everything immediately to free memory
                await context.close()
                await browser.close()
                
                logger.info(f"HTML content successfully saved to {filepath}")
                return filepath, html_content
                
        except Exception as e:
            logger.error(f"Playwright HTML capture failed: {str(e)}")
            return None, None
            
    except Exception as e:
        logger.error(f"Error capturing HTML content: {str(e)}")
        return None, None

def capture_html_content(url):
    """
    Synchronous wrapper for capture_html_content_async.
    
    Args:
        url (str): The URL to capture
        
    Returns:
        tuple: (filepath, html_content) or (None, None) if failed
    """
    try:
        return asyncio.run(capture_html_content_async(url))
    except Exception as e:
        logger.error(f"Error in synchronous HTML capture wrapper: {str(e)}")
        return None, None

def capture_screenshot(url, lottery_type=None):
    """
    Capture HTML content from the specified URL and save metadata to database.
    
    This function captures HTML content using Playwright with Chromium to bypass 
    anti-scraping measures on lottery websites. The function name remains the same
    for compatibility, but it now stores HTML content instead of screenshots.
    
    Args:
        url (str): The URL to capture
        lottery_type (str, optional): The type of lottery. If None, extracted from URL.
        
    Returns:
        str: Path to the saved content file, or None if failed
    """
    # Extract lottery type from URL if not provided
    if not lottery_type:
        lottery_type = extract_lottery_type_from_url(url)
    
    # Run HTML content capture function
    filepath, html_content = capture_html_content(url)
    
    if filepath and html_content:
        # Process the HTML content immediately to extract lottery results
        extracted_data = html_parser.parse_lottery_html(html_content, lottery_type)
        
        # Save content info to database
        screenshot = Screenshot(
            url=url,
            lottery_type=lottery_type,
            timestamp=datetime.utcnow(),
            path=filepath,
            processed=False
        )
        
        db.session.add(screenshot)
        db.session.commit()
        
        logger.info(f"Content capture record saved to database with ID {screenshot.id}")
        
        # Return both the filepath and the extracted data
        return filepath, extracted_data
    
    return None, None

def extract_lottery_type_from_url(url):
    """Extract lottery type from the URL"""
    url_lower = url.lower()
    
    if 'lotto-plus-1' in url_lower:
        return 'Lotto Plus 1'
    elif 'lotto-plus-2' in url_lower:
        return 'Lotto Plus 2'
    elif 'powerball-plus' in url_lower:
        return 'Powerball Plus'
    elif 'powerball' in url_lower:
        return 'Powerball'
    elif 'daily-lotto' in url_lower:
        return 'Daily Lotto'
    elif 'lotto' in url_lower:
        return 'Lotto'
    else:
        return 'Unknown'

def get_unprocessed_screenshots():
    """Get all unprocessed screenshots from the database"""
    return Screenshot.query.filter_by(processed=False).all()

def mark_screenshot_as_processed(screenshot_id):
    """Mark a screenshot as processed in the database"""
    screenshot = Screenshot.query.get(screenshot_id)
    if screenshot:
        screenshot.processed = True
        db.session.commit()
        logger.info(f"Content {screenshot_id} marked as processed")