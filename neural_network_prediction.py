#!/usr/bin/env python3
"""
Neural Network Prediction Integration
Bridges the ML ensemble with the existing prediction workflow
"""

import os
import logging
import psycopg2
import pickle
import numpy as np
from typing import Dict, List, Tuple, Optional
from datetime import datetime

from ml_neural_ensemble import NeuralEnsemble
from ml_training_infrastructure import LotteryMLTrainer
from ml_feature_engineering import LotteryFeatureEngineer
from cross_game_intelligence import (
    get_cross_game_hot_numbers,
    get_cross_game_frequency_boost,
    get_cross_game_intelligence_summary
)
from confidence_calibration import calibrate_prediction_confidence, get_calibrator

logger = logging.getLogger(__name__)

# Cache for trained models (avoid retraining every time)
MODEL_CACHE = {}
CACHE_TIMEOUT_HOURS = 24


def full_ensemble_prediction(lottery_type: str, config: Dict, historical_df) -> Tuple[Optional[List[int]], Optional[List[int]], Optional[float], Optional[str]]:
    """
    FULL ML ENSEMBLE: Random Forest + Gradient Boosting + Neural Network
    with weighted voting and dynamic model adjustment
    """
    try:
        cache_key = f"{lottery_type}_ensemble_{datetime.now().strftime('%Y%m%d')}"
        
        # Check if ensemble is already trained today
        ensemble = None
        if cache_key in MODEL_CACHE:
            cached_data = MODEL_CACHE[cache_key]
            if (datetime.now() - cached_data['timestamp']).total_seconds() < CACHE_TIMEOUT_HOURS * 3600:
                ensemble = cached_data['ensemble']
                logger.info(f"âœ… Using cached ensemble for {lottery_type}")
        
        # Train new ensemble if needed
        if ensemble is None:
            logger.info(f"ðŸ”§ Training fresh ensemble for {lottery_type}...")
            ensemble = NeuralEnsemble(lottery_type, config)
            
            if not ensemble.train_models(historical_df):
                logger.warning("Ensemble training failed, falling back to feature scoring")
                return None, None, None, None
            
            # Cache the trained ensemble
            MODEL_CACHE[cache_key] = {
                'ensemble': ensemble,
                'timestamp': datetime.now()
            }
            logger.info(f"âœ… Ensemble trained and cached")
        
        # Generate prediction using ensemble
        prediction = ensemble.predict(historical_df)
        
        if not prediction:
            logger.warning("Ensemble prediction failed, falling back to feature scoring")
            return None, None, None, None
        
        main_numbers = prediction['main_numbers']
        confidence = prediction['confidence']
        reasoning = prediction['reasoning']
        
        # Handle bonus numbers
        bonus_numbers = []
        if config.get('bonus_count', 0) > 0:
            bonus_numbers = predict_bonus_numbers(
                historical_df,
                config['bonus_range'],
                config['bonus_count']
            )
        
        # â­ CALIBRATE confidence based on historical accuracy
        calibrated_confidence = calibrate_prediction_confidence(lottery_type, confidence, 'ensemble')
        
        logger.info(f"âœ… ENSEMBLE PREDICTION: {main_numbers} + {bonus_numbers}")
        logger.info(f"   Raw confidence: {confidence}% â†’ Calibrated: {calibrated_confidence}%")
        logger.info(f"   Model predictions: {prediction['model_predictions']}")
        
        return main_numbers, bonus_numbers, calibrated_confidence, reasoning
        
    except Exception as e:
        logger.error(f"Full ensemble error: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None


def neural_network_prediction(lottery_type: str, config: Dict) -> Tuple[Optional[List[int]], Optional[List[int]], Optional[float], Optional[str]]:
    """
    Generate lottery prediction using FULL ML ENSEMBLE
    (Random Forest + Gradient Boosting + Neural Network)
    
    Args:
        lottery_type: Type of lottery game
        config: Game configuration dict
    
    Returns:
        Tuple of (main_numbers, bonus_numbers, confidence, reasoning)
    """
    try:
        logger.info(f"ðŸ§  PHASE 2 FULL ENSEMBLE for {lottery_type}...")
        
        # Get historical data
        trainer = LotteryMLTrainer()
        historical_df = trainer.get_historical_draws(lottery_type, days_back=365)
        
        # Smart gating: Try full ensemble if we have enough data (50+ draws)
        # Fall back to feature scoring if data is limited (30-49 draws)
        if historical_df.empty or len(historical_df) < 30:
            logger.info(f"ðŸ“Š Insufficient data ({len(historical_df)} draws). Using Phase 1.")
            return None, None, None, None
        
        # ACTIVATE FULL ML ENSEMBLE for sufficient data
        if len(historical_df) >= 50:
            logger.info(f"ðŸš€ ACTIVATING FULL ML ENSEMBLE ({len(historical_df)} draws available)")
            return full_ensemble_prediction(lottery_type, config, historical_df)
        
        # Use advanced feature engineering to score numbers
        logger.info(f"ðŸ“Š Using Feature-Based Scoring + Cross-Game Intelligence ({len(historical_df)} draws)")
        from ml_feature_engineering import LotteryFeatureEngineer
        feature_engineer = LotteryFeatureEngineer(config)
        
        # Extract all advanced features
        features = feature_engineer.extract_all_features(historical_df)
        
        # Get cross-game intelligence
        cross_game_intel = get_cross_game_intelligence_summary(lottery_type)
        cross_game_hot = set(cross_game_intel.get('cross_game_hot', []))
        
        # Score each number using multiple advanced features + cross-game intelligence
        number_scores = {}
        number_range = range(config['main_range'][0], config['main_range'][1] + 1)
        
        for num in number_range:
            score = 0.0
            
            # Feature 1: Temporal frequency (35% weight - reduced to make room for cross-game)
            score += features.get('temporal_main_freq', {}).get(num, 0) * 0.35
            
            # Feature 2: Recency (18% weight)
            score += features.get('recency_scores', {}).get(num, 0) * 0.18
            
            # Feature 3: Short-term hot status (12% weight)
            if num in features.get('short_term_hot', []):
                score += 0.12
            
            # Feature 4: Statistical momentum (12% weight)
            momentum = features.get('momentum_score', {}).get(num, 0)
            if momentum > 0:
                score += min(momentum * 3, 0.12)  # Cap at 12%
            
            # Feature 5: Co-occurrence associations (8% weight)
            associations = features.get('number_associations', {}).get(num, [])
            score += min(len(associations) / 20.0, 0.08)  # Cap at 8%
            
            # â­ NEW: Feature 6: Cross-game intelligence (15% weight)
            # If hot in related games (LOTTO PLUS when predicting LOTTO), boost it
            if num in cross_game_hot:
                cross_boost = get_cross_game_frequency_boost(lottery_type, num)
                score += cross_boost
                logger.debug(f"   Number {num}: +{cross_boost:.3f} from cross-game intelligence")
            
            number_scores[num] = score
        
        # Select top numbers based on ML scores
        top_numbers = sorted(number_scores.items(), key=lambda x: x[1], reverse=True)[:config['main_count']]
        main_numbers = sorted([num for num, _ in top_numbers])
        
        # Calculate confidence based on score strength and feature agreement
        avg_score = np.mean([number_scores[n] for n in main_numbers])
        score_std = np.std([number_scores[n] for n in main_numbers])
        
        # Realistic ML confidence (2.5-4.0%)
        base_confidence = 2.5
        score_bonus = avg_score * 1.0  # Up to 1% bonus
        consistency_bonus = max(0, 0.5 - score_std)  # Up to 0.5% bonus
        
        confidence = base_confidence + score_bonus + consistency_bonus
        confidence = max(2.5, min(4.0, round(confidence, 1)))
        
        # â­ CALIBRATE confidence based on historical accuracy
        calibrated_confidence = calibrate_prediction_confidence(lottery_type, confidence, 'feature_based')
        
        # Generate reasoning
        hot_count = sum(1 for n in main_numbers if n in features.get('short_term_hot', []))
        cross_game_count = sum(1 for n in main_numbers if n in cross_game_hot)
        reasoning_parts = [
            f"Advanced ML feature scoring ({len(historical_df)} draws analyzed)",
            f"Temporal decay + momentum + cross-game intelligence",
            f"{hot_count} hot numbers, {cross_game_count} cross-game hot",
            f"{calibrated_confidence}% calibrated confidence"
        ]
        reasoning = " | ".join(reasoning_parts)
        
        # Handle bonus numbers
        bonus_numbers = []
        if config.get('bonus_count', 0) > 0:
            bonus_numbers = predict_bonus_numbers(
                historical_df, 
                config['bonus_range'], 
                config['bonus_count']
            )
        
        logger.info(f"âœ… ML Prediction: {main_numbers} + {bonus_numbers}")
        logger.info(f"   Raw confidence: {confidence}% â†’ Calibrated: {calibrated_confidence}%")
        logger.info(f"   Cross-game intelligence: {cross_game_count} numbers boosted")
        
        return main_numbers, bonus_numbers, calibrated_confidence, reasoning
        
    except Exception as e:
        logger.error(f"ML prediction error: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None


def predict_bonus_numbers(historical_df, bonus_range: Tuple[int, int], 
                         bonus_count: int) -> List[int]:
    """
    Predict bonus numbers using frequency analysis
    (Separate from main numbers as bonus pool is different)
    """
    try:
        from collections import Counter
        import random
        
        # Collect all bonus numbers from history
        all_bonus = []
        for _, row in historical_df.iterrows():
            if row['bonus_numbers']:
                all_bonus.extend(row['bonus_numbers'])
        
        if not all_bonus:
            # No history, random selection
            return [random.randint(bonus_range[0], bonus_range[1]) for _ in range(bonus_count)]
        
        # Frequency analysis
        bonus_freq = Counter(all_bonus)
        hot_bonus = [num for num, _ in bonus_freq.most_common(5)]
        
        # Weighted selection (favor hot numbers)
        all_bonus_range = list(range(bonus_range[0], bonus_range[1] + 1))
        weighted_list = []
        
        for num in all_bonus_range:
            if num in hot_bonus:
                weighted_list.extend([num] * 3)  # 3x weight
            else:
                weighted_list.append(num)
        
        # Select bonus numbers
        bonus_numbers = []
        for _ in range(bonus_count):
            if weighted_list:
                selected = random.choice(weighted_list)
                bonus_numbers.append(selected)
                # Remove to avoid duplicates
                weighted_list = [n for n in weighted_list if n != selected]
        
        return sorted(bonus_numbers)
        
    except Exception as e:
        logger.error(f"Error predicting bonus numbers: {e}")
        import random
        return [random.randint(bonus_range[0], bonus_range[1]) for _ in range(bonus_count)]


def validate_and_update_models(lottery_type: str, actual_numbers: List[int], 
                               predicted_numbers: List[int]) -> None:
    """
    Validate prediction against actual results and update model weights
    (Continuous learning)
    """
    try:
        cache_key = f"{lottery_type}_{datetime.now().strftime('%Y%m%d')}"
        
        if cache_key not in MODEL_CACHE:
            logger.info(f"No cached model to update for {lottery_type}")
            return
        
        ensemble = MODEL_CACHE[cache_key]
        
        # This would be called after draw results are available
        # ensemble.update_weights() would adjust model weights based on accuracy
        
        logger.info(f"Model validation and update complete for {lottery_type}")
        
    except Exception as e:
        logger.error(f"Error validating and updating models: {e}")


def train_all_models_fresh():
    """
    Train all models from scratch for all game types
    Useful for periodic retraining or after significant new data
    """
    game_types = ['LOTTO', 'LOTTO PLUS 1', 'LOTTO PLUS 2', 
                  'POWERBALL', 'POWERBALL PLUS', 'DAILY LOTTO']
    
    game_configs = {
        'LOTTO': {'main_count': 6, 'main_range': (1, 52), 'bonus_count': 0},
        'LOTTO PLUS 1': {'main_count': 6, 'main_range': (1, 52), 'bonus_count': 0}, 
        'LOTTO PLUS 2': {'main_count': 6, 'main_range': (1, 52), 'bonus_count': 0},
        'POWERBALL': {'main_count': 5, 'main_range': (1, 50), 'bonus_count': 1, 'bonus_range': (1, 20)},
        'POWERBALL PLUS': {'main_count': 5, 'main_range': (1, 50), 'bonus_count': 1, 'bonus_range': (1, 20)},
        'DAILY LOTTO': {'main_count': 5, 'main_range': (1, 36), 'bonus_count': 0}
    }
    
    results = {}
    
    for game_type in game_types:
        logger.info(f"\n{'='*60}")
        logger.info(f"Training models for {game_type}")
        logger.info(f"{'='*60}")
        
        try:
            config = game_configs[game_type]
            ensemble = NeuralEnsemble(game_type, config)
            
            # Get historical data
            trainer = LotteryMLTrainer()
            historical_df = trainer.get_historical_draws(game_type, days_back=180)
            
            if len(historical_df) < 30:
                logger.warning(f"Insufficient data for {game_type}: {len(historical_df)} draws")
                results[game_type] = 'insufficient_data'
                continue
            
            # Train models
            success = ensemble.train_models(historical_df)
            
            if success:
                # Cache the model
                cache_key = f"{game_type}_{datetime.now().strftime('%Y%m%d')}"
                MODEL_CACHE[cache_key] = ensemble
                results[game_type] = 'success'
                logger.info(f"âœ… Successfully trained {game_type}")
            else:
                results[game_type] = 'training_failed'
                logger.warning(f"âŒ Training failed for {game_type}")
                
        except Exception as e:
            logger.error(f"Error training {game_type}: {e}")
            results[game_type] = f'error: {str(e)}'
    
    logger.info(f"\n{'='*60}")
    logger.info("Training Summary:")
    for game_type, status in results.items():
        logger.info(f"  {game_type}: {status}")
    logger.info(f"{'='*60}\n")
    
    return results


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # Test neural network prediction
    print("Testing Neural Network Prediction System...")
    
    config = {'main_count': 6, 'main_range': (1, 52), 'bonus_count': 0}
    main, bonus, conf, reason = neural_network_prediction('LOTTO', config)
    
    if main:
        print(f"\nPrediction: {main}")
        print(f"Confidence: {conf}%")
        print(f"Reasoning: {reason}")
    else:
        print("Prediction failed")
